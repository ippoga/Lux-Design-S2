{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ver2 learn seccess ep_rew_mean26.8 -> \n# ver3 try action map 2227s with cpu no target_kl\n# ver4 try gpu 740s 483fps 300_000timesteps(same as ver3)\n# ver5 try gpu 6277s 487fps 3M timesteps still no learn\n# ver6 try reward dist_ice gpu 22655s 443fps 10Mtimesteps still no learn\n* pickup on factory when battery<0.1 -- later\n* ice distance smaller when no cargo ice and battery>0.05\n* dig on ice when cargo ice < cargo_cap and battery>0.05 -- later\n## action_masks seems not working ##\n# ver7 try mask cpu 747s 140fps 100_000timesteps seemed learned\n* SB3Wrapper\n* fixed action_masks\n* move=False on ice when no cargo ice and battery>150\n# ver8 gpu 14428s 419fps 6Mtimesteps no learn\n* dist_ice rewards/300\n## realized output must discrete action -> multidiscrete[12,12]##\n# ver9 gpu 2fac no_mask 13976s 433fps 6Mtimesteps ep_rew_mean50.1\n* no target_kl\n# ver12 gpu from ver9best 7272s 421fps 3Mtimesteps ep_rew_mean17.3  wandb\n## not seemed from best##\n# ver 13 gpu from ver9best 10Mtimesteps wandb animate\n\nThis notebook builds up from the [this notebook](https://www.kaggle.com/code/cristojv/updated-rl-with-lux-invalid-action-masking), which trains a simple RL agent using invalid action masking. Rather than having a simple observation space that contains the position of of a single unit (a Heavy Robot), I want to codify the information of whole map. I based the observation space on the one used in [Gym-ÂµRTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning](https://arxiv.org/pdf/2105.13807.pdf). More details about this new space will be given on the following sections.","metadata":{}},{"cell_type":"code","source":"import shutil\nshutil.make_archive('v9best', 'zip', '/kaggle/input/gymver9/logs/action_map/models/best_model/')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:05.854799Z","iopub.execute_input":"2023-07-22T03:45:05.855319Z","iopub.status.idle":"2023-07-22T03:45:05.996545Z","shell.execute_reply.started":"2023-07-22T03:45:05.855282Z","shell.execute_reply":"2023-07-22T03:45:05.995349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport wandb\n# wandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T23:39:24.717517Z","iopub.execute_input":"2023-07-21T23:39:24.718531Z","iopub.status.idle":"2023-07-21T23:39:25.481766Z","shell.execute_reply.started":"2023-07-21T23:39:24.718478Z","shell.execute_reply":"2023-07-21T23:39:25.480496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"]='8b1bc318a40da8800d0980cb0c6b6350e2ee7566'","metadata":{"execution":{"iopub.status.busy":"2023-07-21T23:39:25.48379Z","iopub.execute_input":"2023-07-21T23:39:25.484171Z","iopub.status.idle":"2023-07-21T23:39:25.490848Z","shell.execute_reply.started":"2023-07-21T23:39:25.484137Z","shell.execute_reply":"2023-07-21T23:39:25.489642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n\n!pip install gymnasium\nimport gymnasium\n!pip install --upgrade luxai_s2\n!pip install pettingzoo==1.12.0 stable-baselines3\n!pip install --upgrade \"importlib_metadata<5.0\"\n!pip install sb3_contrib\n# !pip install optuna\nclear_output()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-21T23:39:25.492155Z","iopub.execute_input":"2023-07-21T23:39:25.49248Z","iopub.status.idle":"2023-07-21T23:41:04.055401Z","shell.execute_reply.started":"2023-07-21T23:39:25.492451Z","shell.execute_reply":"2023-07-21T23:41:04.054019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n# __version__ = \"\"\n# this code above is used for Kaggle Notebooks\n# You might not need to run this but if you get an attribute error about the gym package, run it","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-21T23:41:04.060553Z","iopub.execute_input":"2023-07-21T23:41:04.06165Z","iopub.status.idle":"2023-07-21T23:41:04.066366Z","shell.execute_reply.started":"2023-07-21T23:41:04.061607Z","shell.execute_reply":"2023-07-21T23:41:04.065471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import importlib\nimport importlib_metadata\n# kaggle has 6.0.0 installed but we need version <5.0\nimportlib.reload(importlib_metadata)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T23:41:04.06796Z","iopub.execute_input":"2023-07-21T23:41:04.068364Z","iopub.status.idle":"2023-07-21T23:41:04.093787Z","shell.execute_reply.started":"2023-07-21T23:41:04.068331Z","shell.execute_reply":"2023-07-21T23:41:04.092464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remember to ``\"Restart & clear cell outputs\"`` at this time","metadata":{}},{"cell_type":"code","source":"# Common imports \nimport numpy as np\nimport numpy.typing as npt\nimport torch as th\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport os.path as osp\n\nfrom typing import Any, Dict, Callable, Optional\n\n# LuxAI imports\nfrom luxai_s2.env import LuxAI_S2\nfrom lux.config import EnvConfig\nfrom luxai_s2.state import ObservationStateDict\nfrom luxai_s2.unit import UnitType, UnitStateDict, ActionType, BidActionType, FactoryPlacementActionType\nfrom luxai_s2.utils import my_turn_to_place_factory\nfrom luxai_s2.wrappers.controllers import Controller\n\n# Gym imports\nimport gym as gym21\nfrom gymnasium import spaces\nfrom gymnasium import ActionWrapper, ObservationWrapper, RewardWrapper, Wrapper\nfrom gymnasium.wrappers import TimeLimit\n\n# SB3 imports\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecVideoRecorder\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# SB3-contrib imports\nfrom sb3_contrib import MaskablePPO\nfrom sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-22T03:45:15.813959Z","iopub.execute_input":"2023-07-22T03:45:15.814379Z","iopub.status.idle":"2023-07-22T03:45:15.82619Z","shell.execute_reply.started":"2023-07-22T03:45:15.814346Z","shell.execute_reply":"2023-07-22T03:45:15.824895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see from the previous imports we are going to use the **[stable-baselines3-contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)** repo.\n\nThis is the **contrib** package for **Stable-Baselines3** - Experimental reinforcement learning (RL) code.\n\nRemark that this package is experimental and currently does not support Multiprocessing (``SubprocVecEnv`` environments) issue [#49](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/issues/49). Although Multithreading is currently available.\n\nI'm working on a potential fix for supporting multiprocessing to speed up the training time. I will update this notebook acordindly. You can see how can you fix it in the aforementioned issue link.","metadata":{}},{"cell_type":"markdown","source":"# Create a base environment","metadata":{}},{"cell_type":"code","source":"env = LuxAI_S2() # Load environment\nobs = env.reset(12412) # Reset environment -> first observation\nboard = obs['player_0']['board'] # Get board\n\nfig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10,3))\nax[0].imshow(board['ice']) # Show ice\nax[0].set_title(f\"ice: \\n max: {np.max(board['ice'])}, min: {np.min(board['ice'])}\")\nax[1].imshow(board['ore']) # Show ore\nax[1].set_title(f\"ore: \\n max: {np.max(board['ore'])}, min: {np.min(board['ore'])}\")\nax[2].imshow(board['rubble']) # Show rubble\nax[2].set_title(f\"rubble: \\n max: {np.max(board['rubble'])}, min: {np.min(board['rubble'])}\")\nax[3].imshow(np.stack([board['ore'],board['ice'],board['rubble']/100],axis=2))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:15.828513Z","iopub.execute_input":"2023-07-22T03:45:15.828984Z","iopub.status.idle":"2023-07-22T03:45:16.877777Z","shell.execute_reply.started":"2023-07-22T03:45:15.82894Z","shell.execute_reply":"2023-07-22T03:45:16.876411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Single Environment training\n\nThe base notebook focuses on training a single HEAVY robot unit to gather resources. We will do the same here but including action masking.\n\n## 1. Simplifying the action space.\n\nFist we defined an action controller that will: (i) set up a simplified action space (for one robot), (ii) translate actions from the simplified action space to the base action space, (iii) generate action masks, (iv) the heuristic policies for the factory to build one robot ASAP.\n\nFor reference check [StoneTao's](https://www.kaggle.com/code/stonet2000/rl-with-lux-2-rl-problem-solving) notebook and give him an upvote.","metadata":{}},{"cell_type":"code","source":"# Controller class copied here since you won't have access to the luxai_s2 package directly on the competition server\nclass Controller:\n    def __init__(self, action_space: spaces.Space) -> None:\n        self.action_space = action_space\n\n    def action_to_lux_action(self, agent: str, obs: Dict[str, Any], action: npt.NDArray):\n        \"\"\"\n        Takes as input the current \"raw observation\" and the parameterized action and returns\n        an action formatted for the Lux env\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_action_masks(self, agent: str, obs: Dict[str, Any]):\n        \"\"\"\n        Generates a boolean action mask indicating in each discrete dimension whether it would be valid or not\n        \"\"\"\n        raise NotImplementedError()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:16.880374Z","iopub.execute_input":"2023-07-22T03:45:16.881701Z","iopub.status.idle":"2023-07-22T03:45:16.891378Z","shell.execute_reply.started":"2023-07-22T03:45:16.881651Z","shell.execute_reply":"2023-07-22T03:45:16.890198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimpleUnitDiscreteController(Controller):\n    def __init__(self, env_cfg) -> None:\n        \"\"\"\n        A simple controller that controls only the robot that will get spawned.\n        Moreover, it will always try to spawn one heavy robot if there are none regardless of action given\n\n        For the robot unit\n        - 4 cardinal direction movement (4 dims)\n        - a move center no-op action (1 dim)\n        - transfer action just for transferring ice in 4 cardinal directions or center (5)\n        - pickup action for power (1 dims)\n        - dig action (1 dim)\n        - no op action (1 dim) - equivalent to not submitting an action queue which costs power\n\n        It does not include\n        - self destruct action\n        - recharge action\n        - planning (via actions executing multiple times or repeating actions)\n        - factory actions\n        - transferring power or resources other than ice\n\n        To help understand how to this controller works to map one action space to the original lux action space,\n        see how the lux action space is defined in luxai_s2/spaces/action.py\n\n        \"\"\"\n        self.env_cfg = env_cfg\n        self.move_act_dims = 4\n        self.transfer_act_dims = 5\n        self.pickup_act_dims = 1\n        self.dig_act_dims = 1\n        self.no_op_dims = 1\n\n        self.move_dim_high = self.move_act_dims\n        self.transfer_dim_high = self.move_dim_high + self.transfer_act_dims\n        self.pickup_dim_high = self.transfer_dim_high + self.pickup_act_dims\n        self.dig_dim_high = self.pickup_dim_high + self.dig_act_dims\n        self.no_op_dim_high = self.dig_dim_high + self.no_op_dims\n\n        self.total_act_dims = self.no_op_dim_high\n#         action_space = spaces.Discrete(self.total_act_dims)\n        action_space = spaces.MultiDiscrete([12,12])\n        super().__init__(action_space)\n\n    def _is_move_action(self, id):\n        return id < self.move_dim_high\n\n    def _get_move_action(self, id):\n        # move direction is id + 1 since we don't allow move center here\n        return np.array([0, id + 1, 0, 0, 0, 1])\n\n    def _is_transfer_action(self, id):\n        return id < self.transfer_dim_high\n\n    def _get_transfer_action(self, id):\n        id = id - self.move_dim_high\n        transfer_dir = id % 5\n        return np.array([1, transfer_dir, 0, self.env_cfg.max_transfer_amount, 0, 1])\n\n    def _is_pickup_action(self, id):\n        return id < self.pickup_dim_high\n\n    def _get_pickup_action(self, id):\n        return np.array([2, 0, 4, self.env_cfg.max_transfer_amount, 0, 1])\n\n    def _is_dig_action(self, id):\n        return id < self.dig_dim_high\n\n    def _get_dig_action(self, id):\n        return np.array([3, 0, 0, 0, 0, 1])\n\n    def action_to_lux_action(\n        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n    ):\n        shared_obs = obs[agent]\n        lux_action = dict()\n        units = shared_obs[\"units\"][agent]\n        for i,unit_id in enumerate(units.keys()):\n            unit = units[unit_id]\n#             print(self.action_masks(agent=agent,obs=obs))\n#             print(action[:,unit['pos'][0],unit['pos'][1]])\n#             masked = self.action_masks(agent=agent,obs=obs)*action[:,unit['pos'][0],unit['pos'][1]]\n#             masked[masked==0] = -1\n#             print('masked',masked)\n#             choice = np.argmax(masked)\n#             print('choice',choice)\n            choice = action[i]\n            action_queue = []\n            no_op = False\n            if self._is_move_action(choice):\n                action_queue = [self._get_move_action(choice)]\n            elif self._is_transfer_action(choice):\n                action_queue = [self._get_transfer_action(choice)]\n            elif self._is_pickup_action(choice):\n                action_queue = [self._get_pickup_action(choice)]\n            elif self._is_dig_action(choice):\n                action_queue = [self._get_dig_action(choice)]\n            else:\n                # action is a no_op, so we don't update the action queue\n                no_op = True\n\n            # simple trick to help agents conserve power is to avoid updating the action queue\n            # if the agent was previously trying to do that particular action already\n            if len(unit[\"action_queue\"]) > 0 and len(action_queue) > 0:\n                same_actions = (unit[\"action_queue\"][0] == action_queue[0]).all()\n                if same_actions:\n                    no_op = True\n            if not no_op:\n                lux_action[unit_id] = action_queue\n\n#             break\n\n        factories = shared_obs[\"factories\"][agent]\n        if len(units) <= 1:\n            for unit_id in factories.keys():\n                lux_action[unit_id] = 1  # build a single heavy\n#         print('lux_action', agent, lux_action)\n        return lux_action\n\n    def action_masks(self, agent: str, obs: Dict[str, Any]):\n        \"\"\"\n        Defines a simplified action mask for this controller's action space\n\n        Doesn't account for whether robot has enough power\n        \"\"\"\n\n        # compute a factory occupancy map that will be useful for checking if a board tile\n        # has a factory and which team's factory it is.\n        shared_obs = obs[agent]\n        factory_occupancy_map = (\n            np.ones_like(shared_obs[\"board\"][\"rubble\"], dtype=int) * -1\n        )\n        factories = dict()\n        for player in shared_obs[\"factories\"]:\n            factories[player] = dict()\n            for unit_id in shared_obs[\"factories\"][player]:\n                f_data = shared_obs[\"factories\"][player][unit_id]\n                f_pos = f_data[\"pos\"]\n                # store in a 3x3 space around the factory position it's strain id.\n                factory_occupancy_map[\n                    f_pos[0] - 1 : f_pos[0] + 2, f_pos[1] - 1 : f_pos[1] + 2\n                ] = f_data[\"strain_id\"]\n\n        units = shared_obs[\"units\"][agent]\n#         print(units)\n        action_masks = np.zeros((2,self.total_act_dims), dtype=bool)\n        for j, (unit_id, unit) in enumerate(units.items()):\n            action_mask = np.zeros(self.total_act_dims)\n            # movement is always valid\n            action_mask[:4] = True\n            \n            \n            ice_map = shared_obs['board']['ice']\n            if ice_map[unit['pos'][0],unit['pos'][1]]==1 and unit['cargo']['ice']==0 and unit['power']>150:\n                action_mask[:4] = False\n\n            # transferring is valid only if the target exists\n            unit = units[unit_id]\n            pos = np.array(unit[\"pos\"])\n            # a[1] = direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)\n            move_deltas = np.array([[0, 0], [0, -1], [1, 0], [0, 1], [-1, 0]])\n            for i, move_delta in enumerate(move_deltas):\n                transfer_pos = np.array(\n                    [pos[0] + move_delta[0], pos[1] + move_delta[1]]\n                )\n                # check if theres a factory tile there\n                if (\n                    transfer_pos[0] < 0\n                    or transfer_pos[1] < 0\n                    or transfer_pos[0] >= len(factory_occupancy_map)\n                    or transfer_pos[1] >= len(factory_occupancy_map[0])\n                ):\n                    continue\n                factory_there = factory_occupancy_map[transfer_pos[0], transfer_pos[1]]\n                if factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]:\n                    action_mask[\n                        self.transfer_dim_high - self.transfer_act_dims + i\n                    ] = True\n\n            factory_there = factory_occupancy_map[pos[0], pos[1]]\n            on_top_of_factory = (\n                factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]\n            )\n\n            # dig is valid only if on top of tile with rubble or resources or lichen\n            board_sum = (\n                shared_obs[\"board\"][\"ice\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"ore\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"rubble\"][pos[0], pos[1]]\n                + shared_obs[\"board\"][\"lichen\"][pos[0], pos[1]]\n            )\n            if board_sum > 0 and not on_top_of_factory:\n                action_mask[\n                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n                ] = True\n\n            # pickup is valid only if on top of factory tile\n            if on_top_of_factory:\n                action_mask[\n                    self.pickup_dim_high - self.pickup_act_dims : self.pickup_dim_high\n                ] = True\n                action_mask[\n                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n                ] = False\n\n            # no-op is always valid\n            action_mask[-1] = True\n            action_masks[j,:] = action_mask\n#             if np.sum(action_masks)==0:\n#                 action_masks = action_mask\n#             else:\n#                 action_masks = np.concatenate((action_masks, action_mask), axis=0)\n#         print(agent, action_masks)\n#             break\n        return np.array(action_masks, dtype=bool).flatten()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:16.893095Z","iopub.execute_input":"2023-07-22T03:45:16.893466Z","iopub.status.idle":"2023-07-22T03:45:16.946017Z","shell.execute_reply.started":"2023-07-22T03:45:16.893418Z","shell.execute_reply":"2023-07-22T03:45:16.944591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Creating the observation space.\n\nInstead of using ``ObservationWrapper``, which only has information about the single Heavy Robot unit spawned, we will use a new observation space named `RTSObservationWrapper`. Unlike the former, this new space can contain information about all the units contained in the map. The unit space has a shape of\n(map_size, map_size, 13). For a single pixel of the map, a 13 vector component will encode the information about the units that are placed in such pixel.\n\nThis 13-dimensional vector is composed of:\n\n- 2 dimensions that encode the unit type. Since only one Heavy Robot unit is spawned, only two types are considered. The two dimensions encode the following information:\n  - \\[0, 0\\]: No unit is present\n  - \\[1, 0\\]: A factory unit is present\n  - \\[0, 1\\]: A Heavy Robot unit is present\n  - \\[1, 1\\]: A Factory and a Heavy Robot unit are present\n- 10 dimensions that encode in binary the ice in the Heavy Robot's  cargo. For example, [0 ,0 ,0, 0, 0, 0, 0, 0, 1, 1] means that the Heavy Unit contains\n2^1 + 2^0 ice units, 3 units in total.\n- 1 dimensions that indicates if the map pixel contains ice. ","metadata":{}},{"cell_type":"code","source":"# def bin_array(num, m):\n#     \"\"\"Convert a positive integer num into an m-bit bit vector\"\"\"\n#     return np.array(list(np.binary_repr(num).zfill(m))).astype(np.int8)\n\n\nclass RTSObservationWrapper(gymnasium.ObservationWrapper):\n    \"\"\"Each agent just sees its own factories and units.\n\n    Unit type 2 (factory, heavy, noop)\n    Cargo Ice: 10 (heavy) 2**10 \n        cargo/cargomax\n    Resources: 1 (ice)\n\n    For example, a cell where a factory and a heavy robot is located,\n    robot has 1 ice and the cell has no ice:\n\n    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ]\n\n    \"\"\"\n\n    def __init__(self, env: gymnasium.Env) -> None:\n        super().__init__(env)\n        self.observation_space = spaces.Box(\n            0, 1, shape=(5, env.env_cfg.map_size, env.env_cfg.map_size),# dtype=np.int8\n        )\n\n    def observation(self, obs):\n        return RTSObservationWrapper.convert_obs(obs, self.env.state.env_cfg)\n\n    # we make this method static so the submission/evaluation code can use this as well\n    @staticmethod\n    def convert_obs(obs: Dict[str, Any], env_cfg: Any) -> Dict[str, npt.NDArray]:\n        observation = dict()\n        shared_obs = obs[\"player_0\"]\n\n        ice_map = np.array(shared_obs[\"board\"][\"ice\"], dtype=int)\n        ice_map = np.expand_dims(ice_map, axis=0)\n\n        ice_cargo = np.zeros((1, env_cfg.map_size, env_cfg.map_size))\n        battery = np.zeros((1, env_cfg.map_size, env_cfg.map_size))\n\n        for agent in obs.keys():\n            factory_location = np.zeros(\n                shape=(1, env_cfg.map_size, env_cfg.map_size), dtype=np.int8\n            )\n            unit_location = np.zeros(\n                shape=(1, env_cfg.map_size, env_cfg.map_size), dtype=np.int8\n            )\n\n            factories = shared_obs[\"factories\"]['player_0']\n\n            for k in factories.keys():\n                factory = factories[k]\n                fac_x, fac_y = factory[\"pos\"]\n                factory_location[:, fac_x-1:fac_x+2, fac_y-1:fac_y+2] = 1\n\n            units = shared_obs[\"units\"][agent]\n            for k in units.keys():\n                unit = units[k]\n                unit_pos_x, unit_pos_y = unit[\"pos\"]\n\n                unit_location[:, unit_pos_x, unit_pos_y] = 1\n\n#                 unit_ice_cargo = int(unit[\"cargo\"][\"ice\"])\n\n#                 ice_cargo[:, unit_pos_x, unit_pos_y] = bin_array(\n#                     unit_ice_cargo, ice_cargo.shape[0]\n#                 )\n                cargo_space = env_cfg.ROBOTS[unit[\"unit_type\"]].CARGO_SPACE\n                battery_cap = env_cfg.ROBOTS[unit[\"unit_type\"]].BATTERY_CAPACITY\n\n                battery[:, unit_pos_x, unit_pos_y] = unit[\"power\"] / battery_cap,\n                ice_cargo[:, unit_pos_x, unit_pos_y] = unit[\"cargo\"][\"ice\"] / cargo_space,\n\n            agent_observation = np.concatenate(\n                [factory_location, unit_location, ice_map, ice_cargo, battery],\n                axis=0,\n            )\n\n            observation[agent] = agent_observation\n\n        return observation\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:16.948821Z","iopub.execute_input":"2023-07-22T03:45:16.949177Z","iopub.status.idle":"2023-07-22T03:45:16.971324Z","shell.execute_reply.started":"2023-07-22T03:45:16.949147Z","shell.execute_reply":"2023-07-22T03:45:16.96989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Invalid Action Masking\n\nTo perform the invalid action masking we are going to use the ``MaskablePPO`` class implemented in sb3-contrib package.\n\nFrom the sb3-contrib [documentation](https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html).\n\n**Maskable PPO**\n\nImplementation of invalid action masking for the Proximal Policy Optimization (PPO) algorithm. Other than adding support for action masking, the behavior is the same as in SB3âs core PPO algorithm.\n\nTrain a **Maskable PPO** agent requires that the environment has an ``action_masks`` method that returns the invalid action mask. \n\nFor this purpose we extend the``SB3Wrapper`` with the class ``SB3InvalidActionWrapper`` with the corresponding ``action_masks`` method.","metadata":{}},{"cell_type":"code","source":"class SB3Wrapper(gymnasium.Wrapper):\n    def __init__(\n        self,\n        env: LuxAI_S2,\n        bid_policy: Callable[\n            [str, ObservationStateDict], Dict[str, BidActionType]\n        ] = None,\n        factory_placement_policy: Callable[\n            [str, ObservationStateDict], Dict[str, FactoryPlacementActionType]\n        ] = None,\n        controller: Controller = None,\n    ) -> None:\n        \"\"\"\n        A environment wrapper for Stable Baselines 3. It reduces the LuxAI_S2 env\n        into a single phase game and places the first two phases (bidding and factory placement) into the env.reset function so that\n        interacting agents directly start generating actions to play the third phase of the game.\n\n        It also accepts a Controller that translates action's in one action space to a Lux S2 compatible action\n\n        Parameters\n        ----------\n        bid_policy: Function\n            A function accepting player: str and obs: ObservationStateDict as input that returns a bid action\n            such as dict(bid=10, faction=\"AlphaStrike\"). By default will bid 0\n        factory_placement_policy: Function\n            A function accepting player: str and obs: ObservationStateDict as input that returns a factory placement action\n            such as dict(spawn=np.array([2, 4]), metal=150, water=150). By default will spawn in a random valid location with metal=150, water=150\n        controller : Controller\n            A controller that parameterizes the action space into something more usable and converts parameterized actions to lux actions.\n            See luxai_s2/wrappers/controllers.py for available controllers and how to make your own\n        \"\"\"\n        gymnasium.Wrapper.__init__(self, env)\n        self.env = env\n        \n        assert controller is not None\n        \n        # set our controller and replace the action space\n        self.controller = controller\n        self.action_space = controller.action_space\n\n        # The simplified wrapper removes the first two phases of the game by using predefined policies (trained or heuristic)\n        # to handle those two phases during each reset\n        if factory_placement_policy is None:\n            def factory_placement_policy(player, obs: ObservationStateDict):\n                potential_spawns = np.array(\n                    list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n                )\n                spawn_loc = potential_spawns[\n                    np.random.randint(0, len(potential_spawns))\n                ]\n                return dict(spawn=spawn_loc, metal=150, water=150)\n\n        self.factory_placement_policy = factory_placement_policy\n        if bid_policy is None:\n            def bid_policy(player, obs: ObservationStateDict):\n                faction = \"AlphaStrike\"\n                if player == \"player_1\":\n                    faction = \"MotherMars\"\n                return dict(bid=0, faction=faction)\n\n        self.bid_policy = bid_policy\n\n        self.prev_obs = None\n\n    def step(self, action: Dict[str, npt.NDArray]):\n        \n        # here, for each agent in the game we translate their action into a Lux S2 action\n        lux_action = dict()\n        for agent in self.env.agents:\n            if agent in action:\n                lux_action[agent] = self.controller.action_to_lux_action(\n                    agent=agent, obs=self.prev_obs, action=action[agent]\n                )\n            else:\n                lux_action[agent] = dict()\n        \n        # lux_action is now a dict mapping agent name to an action\n        obs, reward, done, info = self.env.step(lux_action)\n        self.prev_obs = obs\n#         print('lux_action',lux_action)\n        return obs, reward, done, done, info\n    \n    def render(self, render_mode='rgb_array'):\n#         return self.env.render('rgb_array')\n        return self.env.render(mode=render_mode)\n\n    def reset(self, **kwargs):\n        # we upgrade the reset function here\n        \n        # we call the original reset function first\n        del kwargs['options']\n        obs = self.env.reset(**kwargs)\n        \n        # then use the bid policy to go through the bidding phase\n        action = dict()\n        for agent in self.env.agents:\n            action[agent] = self.bid_policy(agent, obs[agent])\n        obs, _, _, _ = self.env.step(action)\n#         print(obs['player_0']['board']['rubble'][0][47])\n        \n        # while real_env_steps < 0, we are in the factory placement phase\n        # so we use the factory placement policy to step through this\n        while self.env.state.real_env_steps < 0:\n            action = dict()\n            for agent in self.env.agents:\n                if my_turn_to_place_factory(\n                    obs[\"player_0\"][\"teams\"][agent][\"place_first\"],\n                    self.env.state.env_steps,\n                ):\n                    action[agent] = self.factory_placement_policy(agent, obs[agent])\n                else:\n                    action[agent] = dict()\n            obs, _, _, _ = self.env.step(action)\n        self.prev_obs = obs\n        \n        return obs, 0","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:16.973152Z","iopub.execute_input":"2023-07-22T03:45:16.973516Z","iopub.status.idle":"2023-07-22T03:45:17.002453Z","shell.execute_reply.started":"2023-07-22T03:45:16.973482Z","shell.execute_reply":"2023-07-22T03:45:17.001264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from luxai_s2.wrappers import SB3Wrapper\n\n# class SB3InvalidActionWrapper(SB3Wrapper):\n#     def __init__(\n#         self,\n#         env: LuxAI_S2,\n#         bid_policy: Callable[\n#             [str, ObservationStateDict], Dict[str, BidActionType]\n#         ] = None,\n#         factory_placement_policy: Callable[\n#             [str, ObservationStateDict], Dict[str, FactoryPlacementActionType]\n#         ] = None,\n#         controller: Controller = None,\n#     ) -> None:\n#         super().__init__(env, bid_policy, factory_placement_policy, controller)\n    \n#     def action_masks(self):\n#         return self.controller.action_masks('player_0', self.prev_obs)\n\nclass CustomEnvWrapper(gymnasium.Wrapper):\n    def __init__(self, env: gymnasium.Env) -> None:\n        \"\"\"\n        Adds a custom reward and turns the LuxAI_S2 environment into a single-agent environment for easy training\n        \"\"\"\n        super().__init__(env)\n        self.prev_step_metrics = None\n\n    def step(self, action):\n        agent = \"player_0\"\n        opp_agent = \"player_1\"\n\n#         ice_map = self.env.state.board.ice\n#         ice_tile_locations = np.argwhere(ice_map == 1)\n\n#         units = self.env.state.units[agent]\n#         min_ice_tile_distances = 0\n#         for k in units.keys():\n#             unit = units[k]\n#             min_ice_tile_distances = np.min(\n#                     (ice_tile_locations - np.array(unit_pos)) ** 2, 1\n#                 )\n#         print('min_ice_tile_distances', min_ice_tile_distances)\n#         factories = self.env.state.factories[agent]\n#         for k in factories.keys():\n#             factory = factories[k]\n        \n\n        \n        opp_factories = self.env.state.factories[opp_agent]\n        for k in opp_factories.keys():\n            factory = opp_factories[k]\n            # set enemy factories to have 1000 water to keep them alive the whole around and treat the game as single-agent\n            factory.cargo.water = 1000\n\n        # submit actions for just one agent to make it single-agent\n        # and save single-agent versions of the data below\n        action = {agent: action}\n        obs, _, terminated, truncated, info = self.env.step(action)\n        obs = obs[agent]\n        '''\n        agent_observation = np.concatenate(\n                [factory_location, unit_location, ice_map, ice_cargo, battery],\n                axis=0,\n            ) \n        '''\n#         fac_loc = np.argwhere(obs[0]==1)\n#         unit_loc = np.argwhere(obs[1]==1)[0]\n#         ice_cargo = obs[3,unit_loc[0],unit_loc[1]]\n#         battery = obs[4,unit_loc[0],unit_loc[1]]\n        terminated = terminated[agent]\n        truncated = truncated[agent]\n\n        # we collect stats on teams here. These are useful stats that can be used to help generate reward functions\n        stats: StatsStateDict = self.env.state.stats[agent]\n#         print(stats)\n\n### new reward\n# pickup on factory when battery<0.1 -- later\n# ice distance smaller when no cargo ice and battery>0.05 \n# dig on ice when cargo ice < cargo_cap and battery>0.05 -- later\n#         for unit_id in obs['units']\n\n        info = dict()\n        metrics = dict()\n#         metrics['dist_ice']= min(np.mean((ice_tile_locations - unit_loc) ** 2, 1))\n\n        metrics[\"ice_dug\"] = (\n            stats[\"generation\"][\"ice\"][\"HEAVY\"] + stats[\"generation\"][\"ice\"][\"LIGHT\"]\n        )\n        metrics[\"water_produced\"] = stats[\"generation\"][\"water\"]\n\n        # we save these two to see often the agent updates robot action queues and how often enough\n        # power to do so and succeed (less frequent updates = more power is saved)\n        metrics[\"action_queue_updates_success\"] = stats[\"action_queue_updates_success\"]\n        metrics[\"action_queue_updates_total\"] = stats[\"action_queue_updates_total\"]\n\n        # we can save the metrics to info so we can use tensorboard to log them to get a glimpse into how our agent is behaving\n        info[\"metrics\"] = metrics\n\n        reward = 0\n        if self.prev_step_metrics is not None:\n            # we check how much ice and water is produced and reward the agent for generating both\n            ice_dug_this_step = metrics[\"ice_dug\"] - self.prev_step_metrics[\"ice_dug\"]\n            water_produced_this_step = (\n                metrics[\"water_produced\"] - self.prev_step_metrics[\"water_produced\"]\n            )\n#             to_ice_this_step = 0\n#             if ice_cargo == 0 and battery > 0.05:\n#                 to_ice_this_step = self.prev_step_metrics['dist_ice'] - metrics['dist_ice']\n            \n            # we reward water production more as it is the most important resource for survival\n            reward = ice_dug_this_step / 100 + water_produced_this_step# + to_ice_this_step / 300\n\n        self.prev_step_metrics = copy.deepcopy(metrics)\n        return obs, reward, terminated, truncated, info\n\n    def reset(self, **kwargs):\n        obs,_ = self.env.reset(**kwargs)#[\"player_0\"]\n        obs = obs['player_0']\n        self.prev_step_metrics = None\n        return obs, _","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:17.00393Z","iopub.execute_input":"2023-07-22T03:45:17.004335Z","iopub.status.idle":"2023-07-22T03:45:17.025489Z","shell.execute_reply.started":"2023-07-22T03:45:17.004301Z","shell.execute_reply":"2023-07-22T03:45:17.02421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining a Bid, Factory Placement, and Factory Build\n\nTo test the code above, we can program some heuristic bid and factory placement policies","metadata":{}},{"cell_type":"code","source":"def zero_bid(player, obs):\n    # a policy that always bids 0\n    faction = \"AlphaStrike\"\n    if player == \"player_1\":\n        faction = \"MotherMars\"\n    return dict(bid=0, faction=faction)\n\ndef place_near_random_ice(player, obs):\n    \"\"\"\n    This policy will place a single factory with all the starting resources\n    near a random ice tile\n    \"\"\"\n    if obs[\"teams\"][player][\"metal\"] == 0:\n        return dict()\n    potential_spawns = list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n    potential_spawns_set = set(potential_spawns)\n    done_search = False\n    \n    # simple numpy trick to find locations adjacent to ice tiles.\n    ice_diff = np.diff(obs[\"board\"][\"ice\"])\n    pot_ice_spots = np.argwhere(ice_diff == 1)\n    if len(pot_ice_spots) == 0:\n        pot_ice_spots = potential_spawns\n    \n    # pick a random ice spot and search around it for spawnable locations.\n    trials = 5\n    while trials > 0:\n        pos_idx = np.random.randint(0, len(pot_ice_spots))\n        pos = pot_ice_spots[pos_idx]\n        area = 3\n        for x in range(area):\n            for y in range(area):\n                check_pos = [pos[0] + x - area // 2, pos[1] + y - area // 2]\n                if tuple(check_pos) in potential_spawns_set:\n                    done_search = True\n                    pos = check_pos\n                    break\n            if done_search:\n                break\n        if done_search:\n            break\n        trials -= 1\n    \n    if not done_search:\n        spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n        pos = spawn_loc\n    # this will spawn a factory at pos and with all the starting metal and water\n    metal = obs[\"teams\"][player][\"metal\"]\n    return dict(spawn=pos, metal=100, water=100)\n\ndef factory_build_heavy_robot_policy(obs, agent):\n    for factory_id in obs[agent]['factories'][agent]:\n        if len(obs[agent]['units'][agent]) == 0:\n            return {factory_id : 1}\n    return {}","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:17.027668Z","iopub.execute_input":"2023-07-22T03:45:17.028094Z","iopub.status.idle":"2023-07-22T03:45:17.045079Z","shell.execute_reply.started":"2023-07-22T03:45:17.028059Z","shell.execute_reply":"2023-07-22T03:45:17.044161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_env(env_id: str, rank: int, seed: int = 0, max_episode_steps=100, render_mode='rgb_array'):\n    def _init() -> gymnasium.Env:\n        # verbose = 0\n        # collect stats so we can create reward functions\n        # max factories set to 2 for simplification and keeping returns consistent as we survive longer if there are more initial resources\n        env = gym21.make(env_id, verbose=0, collect_stats=True, MAX_FACTORIES=2)\n\n        # Add a SB3 wrapper to make it work with SB3 and simplify the action space with the controller\n        # this will remove the bidding phase and factory placement phase. For factory placement we use\n        # the provided place_near_random_ice function which will randomly select an ice tile and place a factory near it.\n\n#         env = SB3InvalidActionWrapper(\n        env = SB3Wrapper(\n            env,\n            factory_placement_policy=place_near_random_ice,\n            controller=SimpleUnitDiscreteController(env.env_cfg),\n        )\n        env = RTSObservationWrapper(\n            env\n        )  # changes observation to include a few simple features\n        env = CustomEnvWrapper(env)  # convert to single agent, add our reward\n        env = TimeLimit(\n            env, max_episode_steps=max_episode_steps\n        )  # set horizon to 100 to make training faster. Default is 1000\n        env = Monitor(env)  # for SB3 to allow it to record metrics\n        env.reset(seed=seed + rank)\n        set_random_seed(seed)\n        return env\n\n    return _init","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:17.046692Z","iopub.execute_input":"2023-07-22T03:45:17.047075Z","iopub.status.idle":"2023-07-22T03:45:17.063475Z","shell.execute_reply.started":"2023-07-22T03:45:17.047042Z","shell.execute_reply":"2023-07-22T03:45:17.06236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the agent","metadata":{}},{"cell_type":"code","source":"def animate(imgs, video_name=None, _return=True):\n    # using cv2 to generate videos\n    import cv2\n    import os\n    import string\n    import random\n    video_name = video_name if video_name is not None else ''.join(random.choice(string.ascii_letters) for i in range(18))+'.webm'\n    height, width, layers = imgs[0].shape\n    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'VP80'), 20, (width,height))\n    for img in imgs:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        video.write(img)\n    video.release()\n    if _return:\n        from IPython.display import Video\n        return Video(video_name)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:17.065437Z","iopub.execute_input":"2023-07-22T03:45:17.065832Z","iopub.status.idle":"2023-07-22T03:45:17.078429Z","shell.execute_reply.started":"2023-07-22T03:45:17.065798Z","shell.execute_reply":"2023-07-22T03:45:17.077361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nenv = make_env(\"LuxAI_S2-v0\", 0, 0, max_episode_steps=200, render_mode='human')()\nobs = env.reset() # always reset before starting a new episode!\nimgs = []\nfor i in range(40):\n    action = env.action_space.sample() # sample a random action\n    obs, reward, terminated, truncated, info = env.step(action) # get the new observation and reward\n    imgs += [env.render()] # save to video\n    if terminated or truncated: env.reset()\nenv.close() # close the display window and free up resources\nanimate(imgs, \"random_interaction.webm\") # generate the video replay","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:17.083392Z","iopub.execute_input":"2023-07-22T03:45:17.083772Z","iopub.status.idle":"2023-07-22T03:45:18.918059Z","shell.execute_reply.started":"2023-07-22T03:45:17.083739Z","shell.execute_reply":"2023-07-22T03:45:18.917059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\nclass TensorboardCallback(BaseCallback):\n    def __init__(self, tag: str, verbose=0):\n        super().__init__(verbose)\n        self.tag = tag\n\n    def _on_step(self) -> bool:\n        c = 0\n\n        for i, done in enumerate(self.locals[\"dones\"]):\n            if done:\n                info = self.locals[\"infos\"][i]\n                c += 1\n                for k in info[\"metrics\"]:\n                    stat = info[\"metrics\"][k]\n                    self.logger.record_mean(f\"{self.tag}/{k}\", stat)\n        return True","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:18.919206Z","iopub.execute_input":"2023-07-22T03:45:18.919737Z","iopub.status.idle":"2023-07-22T03:45:18.929449Z","shell.execute_reply.started":"2023-07-22T03:45:18.919706Z","shell.execute_reply":"2023-07-22T03:45:18.928246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Training Setup\n\nNow we can prepare for training by creating training and evaluation environments, as well as defining our algorithm and model.","metadata":{}},{"cell_type":"markdown","source":"To train new observation space with shape (map_size, map_size, 13), a new policy network will be used. Concretely, for our future extractor, the`NatureCNN` network. For the `net_arch`, two dense layers with 64 units and tanh activation will be used.\n\n![](https://stable-baselines3.readthedocs.io/en/master/_images/net_arch.png)","metadata":{}},{"cell_type":"code","source":"class NatureCNN(BaseFeaturesExtractor):\n    \"\"\"\n    CNN from DQN Nature paper:\n        Mnih, Volodymyr, et al.\n        \"Human-level control through deep reinforcement learning.\"\n        Nature 518.7540 (2015): 529-533.\n    :param observation_space:\n    :param features_dim: Number of features extracted.\n        This corresponds to the number of unit for the last layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        observation_space: spaces.Box,\n        features_dim: int = 512,\n    ) -> None:\n        super().__init__(observation_space, features_dim)\n        # We assume CxHxW images (channels first)\n        # Re-ordering will be done by pre-preprocessing or wrapper\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Flatten(),\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n    \npolicy_kwargs = dict(\n    features_extractor_class=NatureCNN,\n    features_extractor_kwargs=dict(features_dim=128),\n    net_arch=(64, 64),\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:18.93108Z","iopub.execute_input":"2023-07-22T03:45:18.93148Z","iopub.status.idle":"2023-07-22T03:45:18.947383Z","shell.execute_reply.started":"2023-07-22T03:45:18.931447Z","shell.execute_reply":"2023-07-22T03:45:18.94619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sb3_contrib import MaskablePPO\nfrom stable_baselines3.ppo import PPO\nfrom sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n\nset_random_seed(42)\nenv_id = 'LuxAI_S2-v0'\nlog_path = \"logs/action_map\"\nn_envs = 4\n\n# set max episode steps to 200 for training environments to train faster\ntrain_max_episode_steps=200\neval_max_episode_steps=200\n\n# set Invalid action masking approach\ninvalid_action_masking=False\n\n# CAUTION: MaskablePPO only works with DummyVecEnv.\n# To implement multiprocessing using SubprocEnv, there is a current issue in GitHub at \n# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/issues/49\n\nenvironments = [make_env(env_id, i, max_episode_steps=train_max_episode_steps) for i in range(n_envs)]\nenv = DummyVecEnv(environments) if invalid_action_masking else DummyVecEnv(environments)\n# env = VecVideoRecorder(env, f\"videos/{run.id}\", render_mode='rgb_array', record_video_trigger=lambda x: x % 2000 == 0, video_length=200)\n\nenv.reset()\n\n# set max episode steps to 1000 to match original environment\neval_environments = [make_env(env_id, i, max_episode_steps=eval_max_episode_steps) for i in range(4)]\neval_env = DummyVecEnv(eval_environments) if invalid_action_masking else DummyVecEnv(eval_environments)\n# eval_env = VecVideoRecorder(eval_env, f\"videos/{run.id}\", render_mode='rgb_array', record_video_trigger=lambda x: x % 2000 == 0, video_length=200)\neval_env.reset()\n\nrollout_steps = 4000\n\nmodel_params = {\n        'policy' : \"MlpPolicy\",\n        'env' : env,\n        'n_steps':rollout_steps // n_envs,\n        'batch_size':800,\n        'learning_rate':3e-4,\n        'policy_kwargs':policy_kwargs,\n        'verbose':1,\n        'n_epochs':2,\n#         'target_kl':0.05,\n        'gamma':0.99,\n#         'tensorboard_log':osp.join(log_path),\n    }\n\nrun = wandb.init(\n    project=\"sb3\",\n    config=model_params,\n    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n    monitor_gym=True,  # auto-upload the videos of agents playing the game\n    save_code=True,  # optional\n)\n\nmodel = MaskablePPO(**model_params) if invalid_action_masking else PPO(tensorboard_log=f\"runs/{run.id}\", **model_params)\n\nprint(\"Training model: {}\".format(type(model)))\n\neval_callback = EvalCallback(\n    eval_env,\n    best_model_save_path=osp.join(log_path, \"models\"),\n    log_path=osp.join(log_path, \"eval_logs\"),\n    eval_freq=24_000,\n    deterministic=False,\n    render=False,\n    n_eval_episodes=5,\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:18.949338Z","iopub.execute_input":"2023-07-22T03:45:18.949838Z","iopub.status.idle":"2023-07-22T03:45:35.073194Z","shell.execute_reply.started":"2023-07-22T03:45:18.949792Z","shell.execute_reply":"2023-07-22T03:45:35.072216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load('v9best', print_system_info=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:47:15.690144Z","iopub.execute_input":"2023-07-22T03:47:15.690553Z","iopub.status.idle":"2023-07-22T03:47:17.926715Z","shell.execute_reply.started":"2023-07-22T03:47:15.690522Z","shell.execute_reply":"2023-07-22T03:47:17.92578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.policy","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:37.439684Z","iopub.execute_input":"2023-07-22T03:45:37.440056Z","iopub.status.idle":"2023-07-22T03:45:39.312647Z","shell.execute_reply.started":"2023-07-22T03:45:37.440023Z","shell.execute_reply":"2023-07-22T03:45:39.311714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  WARNING: This is commented because this Notebook's version will focus on hyperparameter tuning, not on training with fixed params.\nfrom wandb.integration.sb3 import WandbCallback\n\ntotal_timesteps = 10_000_000\nmodel.learn(\n    total_timesteps,\n    callback=[TensorboardCallback(tag=\"train_metrics\"),\n              WandbCallback(gradient_save_freq=100,\n                            model_save_path=f\"models/{run.id}\",\n                            verbose=2,\n                            ),\n            eval_callback],\n)\nmodel.save(osp.join(log_path, \"models/latest_model\"))\nrun.finish()","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:47:28.605776Z","iopub.execute_input":"2023-07-22T03:47:28.606239Z","iopub.status.idle":"2023-07-22T03:47:59.408194Z","shell.execute_reply.started":"2023-07-22T03:47:28.606199Z","shell.execute_reply":"2023-07-22T03:47:59.406909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stats\n\nThis graph shows the episode reward for StoneTao's base training (purple lines) and the invalid action masking (yellow lines).\n\n![image.png](attachment:a28a03ca-96ee-4599-b421-4cae8718ade5.png)\n\nNow let's compare this with the new map-based observation space:\n\n![image.png](attachment:e853f231-40b6-4dba-ae70-b85379884a72.png)\n\nSo far, the performance is significantly worse. Parameter tuning may be needed.","metadata":{},"attachments":{"a28a03ca-96ee-4599-b421-4cae8718ade5.png":{"image/png":"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"},"e853f231-40b6-4dba-ae70-b85379884a72.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"# Tuning hyperparameters\n\nTo optimize the hyperparameters, [Optuna](https://optuna.org/) will be used. To use Optuna we need three things:\n\n- An objective to optimize. The evaluation mean reward in this case.\n- A way to monitored the objective. The objetive is monitored while the Agent is being trained.\n- A way to sample from the hyperparameters and choose the optimal ones.\n\nThese three pieces will be built on the next cell.","metadata":{}},{"cell_type":"code","source":"# import optuna\n# from optuna.pruners import MedianPruner\n# from optuna.samplers import TPESampler\n\n# from sb3_contrib import MaskablePPO\n# from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n\n# ### Clean previous training model and envs ###\n\n# try:\n#     model.env.close()\n#     eval_env.close()\n#     env.close()\n# except Exception:\n#     pass\n\n# ### Global configuration variables ###\n# rollout_steps = 4000\n# env_id = 'LuxAI_S2-v0'\n# log_path = \"logs/exp_mppo\"\n# n_envs = 4\n\n# total_timesteps = 300_000\n\n# eval_max_episode_steps=1000\n# eval_freq=24_000\n\n# invalid_action_masking=True\n\n# n_trials=10 # Optuna will use 10 trials\n# n_startup_trials=3 # Optuna will not use pruning or non-random sampling during these trials\n\n# n_evaluations = int(total_timesteps / eval_freq)  # Number of evaluations per train\n\n# class TrialEvalCallback(EvalCallback):\n#     \"\"\"Monitor the mean eval reward so Optuna can optimize the hyperparameters.\"\"\"\n#     def __init__(\n#         self,\n#         eval_env: gym.Env,\n#         trial: optuna.Trial,\n#         n_eval_episodes: int = 5,\n#         eval_freq: int = 10000,\n#         deterministic: bool = True,\n#         verbose: int = 0,\n#     ):\n#         super().__init__(\n#             eval_env=eval_env,\n#             n_eval_episodes=n_eval_episodes,\n#             eval_freq=eval_freq,\n#             deterministic=deterministic,\n#             verbose=verbose,\n#         )\n#         self.trial = trial\n#         self.eval_idx = 0\n#         self.is_pruned = False\n\n#     def _on_step(self) -> bool:\n#         if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n#             super()._on_step()\n#             self.eval_idx += 1\n#             self.trial.report(self.last_mean_reward, self.eval_idx)\n#             if self.trial.should_prune():\n#                 self.is_pruned = True\n#                 return False\n#         return True\n\n# def sample_ppo_parameters(trial: optuna.Trial):\n#     \"\"\"Get PPO parameters. Some suggested by Optuna based on Trials.\"\"\"\n#     batch_size = 800\n\n#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-03, log=True)\n\n#     n_epochs = trial.suggest_int(\"n_epochs\", 2, 3)\n\n#     max_episode_steps = trial.suggest_int(\"max_episode_steps\", 200, 1000)\n\n#     target_kl = 0.05\n\n#     gamma = 0.99\n\n#     output_paramaters = dict(\n#         learning_rate=learning_rate,\n#         n_epochs=n_epochs,\n#         max_episode_steps=max_episode_steps,\n#         target_kl=target_kl,\n#         gamma=gamma,\n#         batch_size=batch_size,\n#     )\n\n#     return output_paramaters\n\n# def objective(trial: optuna.Trial) -> float:\n#     \"\"\"Set the objective for optuna to optimize.\n    \n#     Notes\n#     -----\n#     In this case, the mean evaluation reward of the agent is optimized.\n    \n#     \"\"\"\n#     set_random_seed(0)\n\n#     policy_kwargs = dict(\n#         features_extractor_class=NatureCNN,\n#         features_extractor_kwargs=dict(features_dim=128),\n#         net_arch=(64, 64),\n#     )\n\n#     ppo_parameters = sample_ppo_parameters(trial)\n\n#     max_episode_steps = ppo_parameters.pop(\"max_episode_steps\")\n\n#     environments = [make_env(env_id, i, max_episode_steps=max_episode_steps) for i in range(n_envs)]\n#     env = DummyVecEnv(environments) if invalid_action_masking else SubprocVecEnv(environments)\n#     env.reset()\n\n#     model = MaskablePPO(\n#         \"CnnPolicy\",\n#         env,\n#         n_steps=rollout_steps // n_envs,\n#         policy_kwargs=policy_kwargs,\n#         verbose=1,\n#         tensorboard_log=osp.join(log_path),\n#         **ppo_parameters,\n#     )\n\n#     eval_environments = [make_env(env_id, i, max_episode_steps=eval_max_episode_steps) for i in range(4)]\n#     eval_env = DummyVecEnv(eval_environments) if invalid_action_masking else SubprocVecEnv(eval_environments)\n#     eval_env.reset()\n\n#     eval_callback = TrialEvalCallback(\n#         eval_env,\n#         trial=trial,\n#         eval_freq=eval_freq,\n#         deterministic=False,\n#         n_eval_episodes=5,\n#     )\n\n#     try:\n#         model.learn(\n#             total_timesteps,\n#             callback=[TensorboardCallback(tag=\"train_metrics\"), eval_callback],\n#         )\n#     finally:\n#         # Free memory.\n#         model.env.close()\n#         eval_env.close()\n#         env.close()\n\n#     return eval_callback.last_mean_reward","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:48.060488Z","iopub.status.idle":"2023-07-22T03:45:48.062838Z","shell.execute_reply.started":"2023-07-22T03:45:48.06246Z","shell.execute_reply":"2023-07-22T03:45:48.062495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sampler = TPESampler(n_startup_trials=n_startup_trials)\n\n# # Do not prune before 1/3 of the max budget is used\n# pruner = MedianPruner(\n#     n_startup_trials=n_startup_trials,\n#     n_warmup_steps=n_evaluations // 3\n# )\n\n# # Create the study and start the hyperparameter optimization\n# study = optuna.create_study(\n#     sampler=sampler, pruner=pruner, direction=\"maximize\"\n# )\n\n# study.optimize(\n#     objective,\n#     n_trials=n_trials,\n#     n_jobs=1,\n#     timeout=None\n# )","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:48.066032Z","iopub.status.idle":"2023-07-22T03:45:48.068027Z","shell.execute_reply.started":"2023-07-22T03:45:48.067632Z","shell.execute_reply":"2023-07-22T03:45:48.067668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Packaging and Submission\n\nWe now have a trained policy. In order to make it submittable to the competition we recommend you write code on separate files and only use kaggle notebooks for training as it can get very messy to program an RL agent just using a Kaggle notebook interface. The starter kit that was downloaded earlier has all of the code above written already and organized into separate files and folders. The observation wrapper and controller written here are saved to the `wrappers` folder. The SB3Wrapper is not in the kit, but is a part of the official luxai_s2 package and you can import it with\n\n```\nfrom luxai_s2.wrappers import SB3Wrapper\n```\n\nThe main file to take note of is `agent.py` which defines your agent's behavior. It will load the policy from`MODEL_WEIGHTS_RELATIVE_PATH` which can be changed at the top of `agent.py`.\n\n`agent.py` also uses the actions_mask function to invalidate some actions so that the policy only generates valid actions, which is a easy way to improve performance.","metadata":{}},{"cell_type":"code","source":"def animate(imgs, _return=True):\n    # using cv2 to generate videos as moviepy doesn't work on kaggle notebooks\n    import cv2\n    import os\n    import string\n    import random\n    video_name = ''.join(random.choice(string.ascii_letters) for i in range(18))+'.webm'\n    height, width, layers = imgs[0].shape\n    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n    video = cv2.VideoWriter(video_name, fourcc, 10, (width,height))\n\n    for img in imgs:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        video.write(img)\n    video.release()\n    if _return:\n        from IPython.display import Video\n        return Video(video_name)\ndef interact(env, agents, steps):\n    # reset our env\n    obs = env.reset(seed=41)\n    np.random.seed(0)\n    imgs = []\n    step = 0\n    # Note that as the environment has two phases, we also keep track a value called \n    # `real_env_steps` in the environment state. The first phase ends once `real_env_steps` is 0 and used below\n\n    # iterate until phase 1 ends\n    while env.state.real_env_steps < 0:\n        if step >= steps: break\n        actions = {}\n        for player in env.agents:\n            o = obs[player]\n            if step == 0:\n                a = agents[player].bid_policy(step, o)\n            else:\n                a = agents[player].factory_placement_policy(step, o)\n            actions[player] = a\n        step += 1\n        obs, rewards, dones, infos = env.step(actions)\n        imgs += [env.render(\"rgb_array\", width=640, height=640)]\n    done = False\n    while not done:\n        if step >= steps: break\n        stats: StatsStateDict = env.state.stats\n#         print(stats)\n        actions = {}\n        for player in env.agents:\n            o = obs[player]\n            a = agents[player].act(step, o)\n            actions[player] = a\n        step += 1\n        obs, rewards, dones, infos = env.step(actions)\n#         print(rewards, dones, infos)\n        imgs += [env.render(\"rgb_array\", width=640, height=640)]\n        done = dones[\"player_0\"] and dones[\"player_1\"]\n    return animate(imgs)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:48.071105Z","iopub.status.idle":"2023-07-22T03:45:48.07317Z","shell.execute_reply.started":"2023-07-22T03:45:48.072769Z","shell.execute_reply":"2023-07-22T03:45:48.072813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile agent.py\n\"\"\"\nThis file is where your agent's logic is kept. Define a bidding policy, factory placement policy, \nas well as a policy for playing the normal phase of the game\n\nThe tutorial will learn an RL agent to play the normal phase and use heuristics for the other two phases.\n\nNote that like the other kits, you can only debug print to standard error e.g. print(\"message\", file=sys.stderr)\n\"\"\"\n\nimport os.path as osp\nimport sys\nimport numpy as np\nimport torch as th\nfrom stable_baselines3.ppo import PPO\nfrom lux.config import EnvConfig\n# from wrappers import SimpleUnitDiscreteController, SimpleUnitObservationWrapper\n\n# change this to use weights stored elsewhere\n# make sure the model weights are submitted with the other code files\n# any files in the logs folder are not necessary. Make sure to exclude the .zip extension here\nMODEL_WEIGHTS_RELATIVE_PATH = \"./logs/action_map/models/best_model.zip\"\nif not os.path.isfile(MODEL_WEIGHTS_RELATIVE_PATH):\n    print('latest')\n    MODEL_WEIGHTS_RELATIVE_PATH = \"./logs/action_map/models/latest_model\"\nelse:\n    print('best')\n    \n\nclass Agent:\n    def __init__(self, player: str, env_cfg: EnvConfig) -> None:\n        self.player = player\n        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n        np.random.seed(0)\n        self.env_cfg: EnvConfig = env_cfg\n\n#         directory = osp.dirname(__file__)\n#         self.policy = PPO.load(osp.join(directory, MODEL_WEIGHTS_RELATIVE_PATH))\n        self.policy = PPO.load(MODEL_WEIGHTS_RELATIVE_PATH)\n#         print(self.policy.policy)\n\n        self.controller = SimpleUnitDiscreteController(self.env_cfg)\n\n    def bid_policy(self, step: int, obs, remainingOverageTime: int = 60):\n        # the policy here is the same one used in the RL tutorial: https://www.kaggle.com/code/stonet2000/rl-with-lux-2-rl-problem-solving\n        return dict(faction=\"AlphaStrike\", bid=0)\n\n    def factory_placement_policy(self, step: int, obs, remainingOverageTime: int = 60):\n        # the policy here is the same one used in the RL tutorial: https://www.kaggle.com/code/stonet2000/rl-with-lux-2-rl-problem-solving\n        if obs[\"teams\"][self.player][\"metal\"] == 0:\n            return dict()\n        potential_spawns = list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n        potential_spawns_set = set(potential_spawns)\n        done_search = False\n\n        ice_diff = np.diff(obs[\"board\"][\"ice\"])\n        pot_ice_spots = np.argwhere(ice_diff == 1)\n        if len(pot_ice_spots) == 0:\n            pot_ice_spots = potential_spawns\n        trials = 5\n        while trials > 0:\n            pos_idx = np.random.randint(0, len(pot_ice_spots))\n            pos = pot_ice_spots[pos_idx]\n\n            area = 3\n            for x in range(area):\n                for y in range(area):\n                    check_pos = [pos[0] + x - area // 2, pos[1] + y - area // 2]\n                    if tuple(check_pos) in potential_spawns_set:\n                        done_search = True\n                        pos = check_pos\n                        break\n                if done_search:\n                    break\n            if done_search:\n                break\n            trials -= 1\n        spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n        if not done_search:\n            pos = spawn_loc\n\n        metal = obs[\"teams\"][self.player][\"metal\"]\n        return dict(spawn=pos, metal=100, water=100)\n\n    def act(self, step: int, obs, remainingOverageTime: int = 60):\n        # first convert observations using the same observation wrapper you used for training\n        # note that SimpleUnitObservationWrapper takes input as the full observation for both players and returns an obs for players\n        raw_obs = dict(player_0=obs, player_1=obs)\n        obs = RTSObservationWrapper.convert_obs(raw_obs, env_cfg=self.env_cfg)\n        obs = obs[self.player]\n        factories = raw_obs[self.player]['factories']\n        for player in factories.keys():\n            for id, factory in factories[player].items():\n                print('step',step, id, factory['cargo'])\n\n        obs = th.from_numpy(obs).float()\n        with th.no_grad():\n\n            # to improve performance, we have a rule based action mask generator for the controller used\n            # which will force the agent to generate actions that are valid only.\n            action_mask = (\n                th.from_numpy(self.controller.action_masks(self.player, raw_obs))\n                .unsqueeze(0)\n                .bool()\n            )\n            \n            # SB3 doesn't support invalid action masking. So we do it ourselves here\n            features = self.policy.policy.features_extractor(obs.unsqueeze(0))\n            x = self.policy.policy.mlp_extractor.policy_net(features)\n            logits = self.policy.policy.action_net(x) # shape (1, N) where N=12 for the default controller\n            logits[action_mask==False] = -1e8 # mask out invalid actions\n            logits = logits.reshape(2,12)\n#             print(logits)\n            dist = th.distributions.Categorical(logits=logits)\n            actions = dist.sample().cpu().numpy() # shape (1, 1)\n#             print(actions)\n\n        # use our controller which we trained with in train.py to generate a Lux S2 compatible action\n        lux_action = self.controller.action_to_lux_action(\n            self.player, raw_obs, actions\n        )\n\n        # commented code below adds watering lichen which can easily improve your agent\n        # shared_obs = raw_obs[self.player]\n        # factories = shared_obs[\"factories\"][self.player]\n        # for unit_id in factories.keys():\n        #     factory = factories[unit_id]\n        #     if 1000 - step < 50 and factory[\"cargo\"][\"water\"] > 100:\n        #         lux_action[unit_id] = 2 # water and grow lichen at the very end of the game\n\n        return lux_action","metadata":{"execution":{"iopub.status.busy":"2023-07-22T04:00:43.970815Z","iopub.execute_input":"2023-07-22T04:00:43.971397Z","iopub.status.idle":"2023-07-22T04:00:44.005245Z","shell.execute_reply.started":"2023-07-22T04:00:43.971356Z","shell.execute_reply":"2023-07-22T04:00:44.003922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recreate our agents and run\nenv = LuxAI_S2()\nobs = env.reset(seed=41)\nagents = {player: Agent(player, env.state.env_cfg) for player in env.agents}\ninteract(env, agents, steps=100)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:59:22.078557Z","iopub.execute_input":"2023-07-22T03:59:22.079613Z","iopub.status.idle":"2023-07-22T03:59:28.534207Z","shell.execute_reply.started":"2023-07-22T03:59:22.07956Z","shell.execute_reply":"2023-07-22T03:59:28.533009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !luxai-s2 main.py main.py -v 2 -s 101 -o replay.html","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:48.09325Z","iopub.status.idle":"2023-07-22T03:45:48.095398Z","shell.execute_reply.started":"2023-07-22T03:45:48.094949Z","shell.execute_reply":"2023-07-22T03:45:48.094994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import IPython # load the HTML replay\n# IPython.display.HTML(filename='replay.html')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:45:48.098656Z","iopub.status.idle":"2023-07-22T03:45:48.100744Z","shell.execute_reply.started":"2023-07-22T03:45:48.100284Z","shell.execute_reply":"2023-07-22T03:45:48.100327Z"},"trusted":true},"execution_count":null,"outputs":[]}]}