{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Jux PPO training based on PureJaxRL https://github.com/luchris429/purejaxrl\n\n> **PureJaxRL is inspired by CleanRL, providing high-quality single-file implementations with research-friendly features. Like CleanRL, this is not a modular library and is not meant to be imported. The repository focuses on simplicity and clarity in its implementations, making it an excellent resource for researchers and practitioners.\n\nWith so much struggle with Jax, it seems training loop somehow loops in a simple environment.\n* one heavy robot per factory.\n* factories are randomly placed adjacent to ice.\n* 12 actions ;\n        [do_nothing,\n        move_up, move_right, move_down, move_left,\n        transfer_ice_center, transfer_ice_up, transfer_ice_right, transfer_ice_down, transfer_ice_left,\n        pickup_power, dig]\n* no factory action except for building heavy at the begining.\n* 9 ovservations :\n        [map.ice,\n          map.rubble/100,\n          unit_map,\n          unit_power_map/3000,\n          unit_cargo_ice_map/1000,\n          enemy_unit_map,\n          factory_occupancy_map/127,\n          factory_power_map/5000,\n          enemy_factory_occupancy_map/127]\n\n\nNetwork is based on https://github.com/RoboEden/lux2-deimos/tree/372a430c8ca06454f6591e9f5d4b0ab505420fea\n\njax.jit the whole process need some amendments, but so far I found no improvement in speed with jitting.\n\nUsing fixed valid_spawn_mask.\n\nAction maskings(e.g. No dig on factory) are mapped and stacked with observation, then transferred to network.\n\nI don't understand and still learning the PPO part, calculate loss, gradient and update. Not sure about logprob of multi agent actions and just summed.\n\nNeed restarting after installing parts.","metadata":{}},{"cell_type":"code","source":"import optax # need to import here before numpy1.25.2installing","metadata":{"execution":{"iopub.status.busy":"2023-09-30T22:34:45.176167Z","iopub.execute_input":"2023-09-30T22:34:45.176662Z","iopub.status.idle":"2023-09-30T22:34:45.182084Z","shell.execute_reply.started":"2023-09-30T22:34:45.176617Z","shell.execute_reply":"2023-09-30T22:34:45.180725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numpy==1.25.2 # to install distrax and gymnax","metadata":{"execution":{"iopub.status.busy":"2023-09-30T22:34:45.184332Z","iopub.execute_input":"2023-09-30T22:34:45.185258Z","iopub.status.idle":"2023-09-30T22:34:53.643147Z","shell.execute_reply.started":"2023-09-30T22:34:45.185224Z","shell.execute_reply":"2023-09-30T22:34:53.641864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install distrax\n!pip install gymnax","metadata":{"execution":{"iopub.status.busy":"2023-09-30T22:34:53.644915Z","iopub.execute_input":"2023-09-30T22:34:53.645326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to use a lower version of jax for environment generation. Does not affect normal RL inference code\n%pip install --upgrade \"jax[cuda11_cudnn82]==0.4.7\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n# install the package. For now this is from source, the final pypi package will be released soon\n%pip install git+https://github.com/RoboEden/jux.git@dev\n# rich is needed by this tutorial for nice printing\n%pip install rich\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: At the moment you will need to restart the notebook for the code to work. For those who train on Kaggle notebooks, we recommend you to download the model and submission after training as automatic submission from Kaggle notebook may not work.","metadata":{}},{"cell_type":"code","source":"# !pip install numpy==1.22.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install numpy==1.21","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip uninstall -y jax","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install jax==0.4.14 # for orbax.checkpoint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\nimport numpy as np\nimport flax.linen as nn\n# from flax.linen.initializers import constant, orthogonal\nfrom typing import Sequence, NamedTuple, Any\nfrom flax.training.train_state import TrainState","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import distrax\nimport gymnax\n# from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\nfrom gymnax.environments import environment, spaces\nfrom flax import struct\nimport rich","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jax.lib import xla_bridge\nprint(xla_bridge.get_backend().platform) # to check if gpu is used. Sometimes not after Keyboard Interrupt or in some occasions.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jux.env import JuxEnv\nfrom jux.config import JuxBufferConfig, EnvConfig\nfrom jux.actions import JuxAction\n\njux_env = JuxEnv(\n    env_cfg=EnvConfig(),\n    buf_cfg=JuxBufferConfig(MAX_N_UNITS=200),\n)\n\n# jux_env.buf_cfg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msize = jux_env.buf_cfg.MAP_SIZE\nmax_n_fac = jux_env.buf_cfg.MAX_N_FACTORIES\nmax_n_units = jux_env.buf_cfg.MAX_N_UNITS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jux.env import JuxEnvBatch\n# jux_env_batch = JuxEnvBatch(env_cfg=EnvConfig(max_episode_length=40),buf_cfg=JuxBufferConfig(MAX_N_UNITS=200))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SELayer(nn.Module):\n    channel: int\n    reduction: int = 4#16\n\n    @nn.compact\n    def __call__(self, x):\n        b, h, w, c = x.shape\n        y = jnp.mean(x, axis=(-3, -2), keepdims=True)\n        y = nn.Dense(self.channel // self.reduction)(y)\n        y = nn.relu(y)\n        y = nn.Dense(self.channel)(y)\n        y = nn.sigmoid(y)\n        y = jnp.transpose(y,(0,3,1,2))\n        y = jnp.tile(y,(h,w))\n        y = jnp.transpose(y,(0,2,3,1))\n        return x * y\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    in_channel: int\n    out_channel: int\n    kernel_size: int = 5\n    padding: int = 2\n\n    @nn.compact\n    def __call__(self, x):\n        left = nn.Conv(self.out_channel, kernel_size=(self.kernel_size, self.kernel_size), padding=(self.padding, self.padding))(x)\n        left = nn.leaky_relu(left)\n        left = nn.Conv(self.out_channel, kernel_size=(self.kernel_size, self.kernel_size), padding=(self.padding, self.padding))(left)\n        left = nn.leaky_relu(left)\n        selayer = SELayer(self.out_channel)\n        out = selayer(left)\n        out = out + x\n        out = nn.leaky_relu(out)\n        return out\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    features: int\n    num_act: int\n\n    @nn.compact\n    def __call__(self, map_feature, unit_pos):\n        B, H, W, C = map_feature.shape\n        n_mask = C - self.features\n        action_mask = map_feature[:,:,:,-n_mask:]\n        map_feature = map_feature[:,:,:,:-n_mask]\n        x = nn.Conv(features=self.features, kernel_size=(1, 1))(map_feature)\n        x = nn.leaky_relu(x)\n        x = nn.Conv(features=self.features, kernel_size=(1, 1))(x)\n        x = nn.leaky_relu(x)\n\n        for _ in range(4):\n            x = ResidualBlock(self.features, self.features)(x)\n\n        BN = nn.BatchNorm(use_running_average=False)\n        variables = BN.init(jax.random.split(rng)[0], x)\n        x,_ = BN.apply(variables,x,mutable=['batch_stats'])\n        actor_mean = x.at[\n            jnp.repeat(jnp.arange(len(x)),len(unit_pos[0])).reshape(-1,len(unit_pos[0])),\n            unit_pos[:,:,0],\n            unit_pos[:,:,1],\n            :].get()\n        actor_mean = nn.Conv(\n            features=self.num_act, kernel_size=(1,1)\n        )(actor_mean)\n        action_mask = action_mask.at[\n            jnp.repeat(jnp.arange(len(x)),len(unit_pos[0])).reshape(-1,len(unit_pos[0])),\n            unit_pos[:,:,0],\n            unit_pos[:,:,1],\n            :].get()\n        actor_mean = actor_mean - action_mask\n        pi = distrax.Categorical(logits=actor_mean)\n        x = nn.Conv(features=self.features, kernel_size=(1, 1))(x)\n        x = nn.leaky_relu(x)\n        critic_value = nn.Conv(features=1, kernel_size=(1, 1))(x)\n        critic_value = jnp.mean(critic_value, axis=(-3, -2, -1))\n\n        return pi, critic_value\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transition(NamedTuple):\n    done: jnp.ndarray\n    action: jnp.ndarray\n    value: jnp.ndarray\n    reward: jnp.ndarray\n    log_prob: jnp.ndarray\n    obs: jnp.ndarray\n    last_unipos: jnp.ndarray\n    info: jnp.ndarray\n\n@struct.dataclass\nclass LogEnvState:\n    env_state: environment.EnvState\n    episode_returns: float\n    episode_lengths: int\n    returned_episode_returns: float\n    returned_episode_lengths: int\n    timestep: int\n    ice_got: float\n    episode_water: float\n    cargo_water: float\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from jux.map.position import Position\n\nradius = 6\ndelta_xy = jnp.mgrid[-radius:radius + 1, -radius:radius + 1]  # int[2, 13, 13]\ndelta_xy = jnp.array(jnp.nonzero(jnp.abs(delta_xy[0]) + jnp.abs(delta_xy[1]) <= radius)).T  # int[85, 2]\ndelta_xy = delta_xy - jnp.array([radius, radius])\ndelta_xy = delta_xy.astype(Position.dtype())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_train(config, raw_restored=None):\n    config[\"NUM_UPDATES\"] = (\n        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n    )\n    config[\"MINIBATCH_SIZE\"] = (\n        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n    )\n    rich.print('config',config)\n\n    def linear_schedule(count):\n        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n        return config[\"LR\"] * frac\n\n    def train(rng):\n\n        # INIT NETWORK\n        network = Net(features=num_features, num_act=num_act)\n        if raw_restored is None:\n            rng, _rng = jax.random.split(rng)\n            init_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n            init_x = jnp.expand_dims(jnp.zeros((msize,msize,num_features+num_act)),axis=0)\n            network_params = network.init(_rng,\n                                          init_x,\n                                          unit_pos=jnp.expand_dims(jnp.zeros((n_train_units,2),dtype='int8'),axis=0))\n        else:\n            network_params = raw_restored['runner_state'][0]['params']\n        if config[\"ANNEAL_LR\"]:\n            tx = optax.chain(\n                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n            )\n        else:\n            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n        train_state = TrainState.create(\n            apply_fn=network.apply,\n            params=network_params,\n            tx=tx,\n        )\n        # INIT ENV\n        def init_env(rng, render=False):\n            rng, _rng = jax.random.split(rng)\n            reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n            env_state = jux_env_batch.reset(reset_rng[:,0])\n            rich.print('factories_per_team',env_state.board.factories_per_team)\n            batch_size = config[\"NUM_ENVS\"]\n            bid = jnp.zeros((batch_size, 2), dtype=jnp.int32)\n            faction = jnp.zeros((batch_size, 2), dtype=jnp.int8)\n\n            env_state, (observations, rewards, dones, infos) = jux_env_batch.step_bid(env_state, bid, faction)\n            water = jnp.ones((batch_size, 2), dtype=jnp.int32) * 150\n            metal = jnp.ones((batch_size, 2), dtype=jnp.int32) * 150\n            ice_maps = env_state.board.map.ice\n            ore_maps = env_state.board.map.ore\n            def around_ice_mask(ice_map):\n                return jnp.zeros((msize,msize)).at[:,:-2].add(ice_map[:,2:]).at[1:,:-2].add(ice_map[:-1,2:]).at[:-1,:-2].add(ice_map[1:,2:])\n            def select_spawn(ice_maps, vsm, rng):\n                around_ice1 = around_ice_mask(ice_maps)\n                around_ice2 = jnp.rot90(around_ice_mask(jnp.rot90(ice_maps,1)),-1)\n                around_ice3 = jnp.rot90(around_ice_mask(jnp.rot90(ice_maps,2)),-2)\n                around_ice4 = jnp.rot90(around_ice_mask(jnp.rot90(ice_maps,3)),-3)\n                around_ices = (around_ice1+around_ice2+around_ice3+around_ice4)*vsm\n                rng, subkey = jax.random.split(rng)\n                random_idx = jax.random.randint(subkey, (2,),0,jnp.sum(jnp.bool_(around_ices)))\n                vsm_random_idx = jax.random.randint(subkey, (2,),0,jnp.sum(vsm))\n                return jnp.array([jnp.argwhere(around_ices,size=150)[random_idx[0]], jnp.argwhere(vsm,size=3000)[vsm_random_idx[0]]])\n#             factories_per_team = env_state.board.factories_per_team[0]\n            def valid_spawn_mask(ice_map, ore_map, factory_pos):\n                valid_spawns_mask = (~ice_map & ~ore_map)  # bool[..., height, width]\n                valid_spawns_mask = valid_spawns_mask & jnp.roll(valid_spawns_mask, 1, axis=-2)\n                valid_spawns_mask = valid_spawns_mask & jnp.roll(valid_spawns_mask, -1, axis=-2)\n                valid_spawns_mask = valid_spawns_mask & jnp.roll(valid_spawns_mask, 1, axis=-1)\n                valid_spawns_mask = valid_spawns_mask & jnp.roll(valid_spawns_mask, -1, axis=-1)\n                valid_spawns_mask = valid_spawns_mask.at[..., [0, -1], :].set(False)\n                valid_spawns_mask = valid_spawns_mask.at[..., :, [0, -1]].set(False)\n\n#                 batch_shape = valid_spawns_mask.shape[:-2]\n                factory_overlap = factory_pos[..., None, :] + delta_xy  # int[..., 2 * MAX_N_FACTORIES, 85, 2]\n                factory_overlap = factory_overlap.reshape((-1, 2))  # int[..., 2 * MAX_N_FACTORIES * 85, 2]\n                factory_overlap = jnp.clip(factory_overlap, 0, jnp.array([msize - 1, msize - 1]))\n\n#                 batch_idx = tuple(slice(None, b) for b in batch_shape)\n                valid_spawns_mask = valid_spawns_mask.at[(factory_overlap[..., 0], factory_overlap[..., 1])]\\\n                                                     .set(False,mode='drop')\n                return valid_spawns_mask\n\n            def spawn_step(env_state, unused):\n                vsm = jax.vmap(valid_spawn_mask)(ice_maps, ore_maps, env_state.board.factory_pos)\n                spawn = jax.vmap(select_spawn)(ice_maps, vsm, spawn_rng)#[:,:,1:]\n                # jax.debug.print(\"spawn: {}, vsm_sum:{}\", spawn[0], vsm[0].sum())\n                env_state, (observations, rewards, dones, infos) = jux_env_batch.step_factory_placement(env_state, spawn, water, metal)\n                return env_state, env_state\n            spawn_rng = jax.random.split(rng, batch_size)\n            env_state, env_states = jax.lax.scan(spawn_step, env_state, None, 2*n_train_units)\n            build_heavy = jux_action._replace(factory_action=(jnp.ones((batch_size,2,max_n_fac),dtype='int8')*-1).at[:,:,:].set(1))\n            env_state, (observations, reward, done, info) = jux_env_batch.step_late_game(env_state, build_heavy)\n\n            if render:\n                import matplotlib.pyplot as plt\n                fig, axes = plt.subplots(1,3, tight_layout=True, figsize=(14,5))\n                axes = axes.flatten()\n                for i in range(len(axes)):\n                    env0 = jax.tree_map(lambda x: x[i,], env_state)\n                    img = jux_env.render(env0, \"rgb_array\")\n                    axes[i].axis('off')\n                    axes[i].set_title(i)\n                    axes[i].imshow(img)\n                plt.show()\n            state = LogEnvState(env_state,\n                              jnp.zeros(batch_size,),\n                              jnp.zeros(batch_size,),\n                              jnp.zeros(batch_size,),\n                              jnp.zeros(batch_size,),\n                              jnp.zeros(batch_size,),\n                              jnp.zeros(batch_size,),\n                              jnp.zeros(batch_size,),\n                              cargo_water=jnp.sum(env_state.factories.cargo.water, axis=2, dtype='float32')[:,0],\n                              )\n            return state\n        render = (jit==False)\n        state = init_env(rng, render=render)\n\n        def obsv_from_state(env_state):\n            facocc_map = env_state.board.factory_occupancy_map\n            fac_id = env_state.factories.unit_id[:,0]\n            enemy_fac_id = env_state.factories.unit_id[:,1]\n            def new_facocc_map_fn(facocc_map, fac_id):\n                def new_facocc_map_i(i,facocc_map):\n                    new_facocc_map = jnp.where(facocc_map==fac_id[i],127,facocc_map)\n                    return new_facocc_map\n                new_facocc_map = jax.lax.fori_loop(0,max_n_fac,new_facocc_map_i,facocc_map)\n                return new_facocc_map\n            new_facocc_map = jax.vmap(new_facocc_map_fn)(facocc_map,enemy_fac_id)\n            # ###rich.print('new_facocc_map',new_facocc_map)\n            enemy_facocc_map = jax.vmap(new_facocc_map_fn)(facocc_map,fac_id)\n            # ###rich.print('enemy_facocc_map',enemy_facocc_map)\n            facpower = env_state.factories.power[:,0]\n            def facpower_map_fn(one_map, fac_id, facpower):\n                def facpower_map_i(i,one_map):\n                    return jnp.where(one_map==fac_id[i],facpower[i],one_map)\n                return jax.lax.fori_loop(0,max_n_fac,facpower_map_i,one_map)\n            facpower_map = jnp.array(jnp.where(new_facocc_map==127,-1,new_facocc_map),dtype='int32')\n            facpower_map = jax.vmap(facpower_map_fn)(facpower_map,fac_id,facpower)\n            # ###rich.print('facpower_map',facpower_map)\n\n            new_unit_map = jnp.zeros((batch_size,msize,msize)).at[\n              jnp.tile(jnp.arange(batch_size)[:,jnp.newaxis],(1,max_n_units)),\n              env_state.units.pos.pos[:,0,:,0].reshape(batch_size,max_n_units),\n              env_state.units.pos.pos[:,0,:,1].reshape(batch_size,max_n_units),\n                                                          ].set(1)\n            enemy_unit_map = jnp.zeros((batch_size,msize,msize)).at[\n              jnp.tile(jnp.arange(batch_size)[:,jnp.newaxis],(1,max_n_units)),\n              env_state.units.pos.pos[:,1,:,0].reshape(batch_size,max_n_units),\n              env_state.units.pos.pos[:,1,:,1].reshape(batch_size,max_n_units),\n                                                          ].set(1)\n            # ###rich.print('new_unit_map',new_unit_map.sum(),new_unit_map)\n            unit_power_map = jnp.zeros((batch_size,msize,msize)).at[\n              jnp.tile(jnp.arange(batch_size)[:,jnp.newaxis],(1,max_n_units)),\n              env_state.units.pos.pos[:,0,:,0].reshape(batch_size,max_n_units),\n              env_state.units.pos.pos[:,0,:,1].reshape(batch_size,max_n_units),\n                                                          ].set(\n                                                              env_state.units.power[:,0,:].reshape(batch_size,max_n_units))\n            # ###rich.print('unit_power_map',unit_power_map[0])\n            enemy_unit_power_map = jnp.zeros((batch_size,msize,msize)).at[\n              jnp.tile(jnp.arange(batch_size)[:,jnp.newaxis],(1,max_n_units)),\n              env_state.units.pos.pos[:,1,:,0].reshape(batch_size,max_n_units),\n              env_state.units.pos.pos[:,1,:,1].reshape(batch_size,max_n_units),\n                                                          ].set(\n                                                              env_state.units.power[:,1,:].reshape(batch_size,max_n_units))\n            # ###rich.print('unit_power_map',unit_power_map)\n            unit_ice_map = jnp.zeros((batch_size,msize,msize)).at[\n              jnp.tile(jnp.arange(batch_size)[:,jnp.newaxis],(1,max_n_units)),\n              env_state.units.pos.pos[:,0,:,0].reshape(batch_size,max_n_units),\n              env_state.units.pos.pos[:,0,:,1].reshape(batch_size,max_n_units),\n                                                          ].set(\n                                                              env_state.units.cargo.stock[:,0,:,0].reshape(batch_size,max_n_units))\n            # ###rich.print('unit_ice_map',unit_ice_map[0])\n            action0_move0_mask_map = jnp.zeros((batch_size,msize,msize))\n            action1_move1_mask_map = jnp.zeros((batch_size,msize,msize))\n            action2_move2_mask_map = jnp.zeros((batch_size,msize,msize))\n            action3_move3_mask_map = jnp.zeros((batch_size,msize,msize))\n            action4_move4_mask_map = jnp.zeros((batch_size,msize,msize))\n            action5_transfer0_mask_map = jnp.where(new_facocc_map==127,jnp.inf,0)\n            action6_transfer1_mask_map = jnp.zeros((batch_size,msize,msize)).at[\n              :,:,0].add(jnp.inf).at[\n                  :,:,1:].add(action5_transfer0_mask_map[:,:,:-1])\n            action7_transfer2_mask_map = jnp.zeros((batch_size,msize,msize)).at[\n              :,-1,:].add(jnp.inf).at[\n                  :,:-1,:].add(action5_transfer0_mask_map[:,1:,:])\n            action8_transfer3_mask_map = jnp.zeros((batch_size,msize,msize)).at[\n              :,:,-1].add(jnp.inf).at[\n                  :,:,:-1].add(action5_transfer0_mask_map[:,:,1:])\n            action9_transfer4_mask_map = jnp.zeros((batch_size,msize,msize)).at[\n              :,0,:].add(jnp.inf).at[\n                  :,1:,:].add(action5_transfer0_mask_map[:,:-1,:])\n            action10_pickup_mask_map = jnp.where(new_facocc_map==127,jnp.inf,0)\n            action11_dig_mask_map = jnp.where(new_facocc_map==127,0,jnp.inf)\n            return jnp.stack((env_state.board.map.ice,\n                             env_state.board.map.rubble/100,\n                             new_unit_map,\n                             unit_power_map/3000,\n                             unit_ice_map/1000,\n                             enemy_unit_map,\n                             new_facocc_map/127,\n                             facpower_map/5000,\n                             enemy_facocc_map/127,\n                             action0_move0_mask_map,\n                             action1_move1_mask_map,\n                             action2_move2_mask_map,\n                             action3_move3_mask_map,\n                             action4_move4_mask_map,\n                             action5_transfer0_mask_map,\n                             action6_transfer1_mask_map,\n                             action7_transfer2_mask_map,\n                             action8_transfer3_mask_map,\n                             action9_transfer4_mask_map,\n                             action10_pickup_mask_map,\n                             action11_dig_mask_map,\n                            ),axis=3)\n\n        # TRAIN LOOP\n        def _update_step(runner_state, unused, rng=rng):\n            # COLLECT TRAJECTORIES\n\n\n            def _env_step(runner_state, unused):\n                train_state, state, last_obs, rng = runner_state\n                last_unipos = state.env_state.units.pos.pos\n\n                # SELECT ACTION\n                rng, _rng = jax.random.split(rng)\n                pi, value = network.apply(train_state.params,\n                                          last_obs,\n                                          state.env_state.units.pos.pos[:,0,:n_train_units],\n                                          )\n                ###rich.print('pi,value',value)\n                action = pi.sample(seed=_rng)[:,:n_train_units]\n#                 action = multi_action[:,0]\n                #rich.print('action',action)\n                # jax.debug.print(\"action: {}\", action)\n                # jax.debug.print(\"value: {}\", value)\n\n#                 log_prob = pi.log_prob(multi_action)[:,0]\n                log_prob = pi.log_prob(action)[:,:n_train_units].sum(-1)\n                # log_prob = pi.log_cdf(action)\n                # #rich.print('log_p_cdf',log_prob)\n                # #rich.print('log_p_',pi.log_prob(action))\n                # jax.debug.print(\"log_prob: {}\", log_prob)\n\n                # STEP ENV\n                # water_act = jux_action._replace(factory_action=jux_action.factory_action.at[:,:,:2].set(2))\n                action_type = jnp.array([0,0,0,0,0,1,1,1,1,1,2,3],dtype='int8')\n                direction = jnp.array([0,1,2,3,4,0,1,2,3,4,0,0],dtype='int8')\n                resource = jnp.array([0,0,0,0,0,0,0,0,0,0,4,0],dtype='int8')\n                amount = jnp.array([0,0,0,0,0,1000,1000,1000,1000,1000,3000,0],dtype='int16')\n                update_q = jnp.array([0,1,1,1,1,1,1,1,1,1,1,1],dtype='bool')\n                sample_action_type = jax.tree_map(lambda x: action_type[x], action)\n                sample_direction = jax.tree_map(lambda x: direction[x], action)\n                sample_resource = jax.tree_map(lambda x: resource[x], action)\n                sample_amount = jax.tree_map(lambda x: amount[x], action)\n                sample_update = jax.tree_map(lambda x: update_q[x], action)\n                sample_unit_action_queue = jux_action.unit_action_queue._replace(\n                    action_type=jux_action.unit_action_queue.action_type.at[:,0,:n_train_units,0].set(sample_action_type),\n                    direction=jux_action.unit_action_queue.direction.at[:,0,:n_train_units,0].set(sample_direction),\n                    resource_type=jux_action.unit_action_queue.resource_type.at[:,0,:n_train_units,0].set(sample_resource),\n                    amount=jux_action.unit_action_queue.amount.at[:,0,:n_train_units,0].set(sample_amount),\n                    n=jux_action.unit_action_queue.n.at[:,0,:n_train_units].set(1),\n                )\n                sample_action = jux_action._replace(unit_action_queue=sample_unit_action_queue,\n                                                    unit_action_queue_update=jux_action.unit_action_queue_update.at[:,0,:n_train_units].set(sample_update),\n                                                    unit_action_queue_count=jux_action.unit_action_queue_count.at[:,0,:n_train_units].set(1),\n                )\n                env_state, (observations, reward, done, info) = jux_env_batch.step_late_game(state.env_state, sample_action)\n\n                ice_got = jnp.sum(env_state.units.cargo.ice, axis=2, dtype='float32')[:,0]\n                factory_ice_got = jnp.sum(env_state.factories.cargo.ice, axis=2, dtype='float32')[:,0]\n                ice_got += factory_ice_got\n                cargo_water = jnp.sum(env_state.factories.cargo.water, axis=2, dtype='float32')[:,0]\n                water_produced = cargo_water - state.cargo_water + env_state.n_factories[:,0]\n                done = done[:,0]\n                # reward = reward[:,0] + 0.2 + ice_got - state.ice_got\n                reward = ice_got - state.ice_got + 10*water_produced\n                new_episode_return = state.episode_returns + reward\n                new_episode_length = state.episode_lengths + 1\n                episode_water = water_produced + state.episode_water\n                state = LogEnvState(\n                    env_state=env_state,\n                    episode_returns=new_episode_return * (1 - done),\n                    episode_lengths=new_episode_length * (1 - done),\n                    returned_episode_returns=state.returned_episode_returns * (1 - done)\n                    + new_episode_return * done,\n                    # + new_episode_return,\n                    returned_episode_lengths=state.returned_episode_lengths * (1 - done)\n                    + new_episode_length * done,\n                    timestep=state.timestep + 1,\n                    ice_got=ice_got,\n                    cargo_water=cargo_water,\n                    episode_water=episode_water,\n                )\n                info[\"returned_episode_returns\"] = state.returned_episode_returns\n                info[\"returned_episode_lengths\"] = state.returned_episode_lengths\n                info[\"timestep\"] = state.timestep\n                info[\"returned_episode\"] = done\n                info[\"new_episode_return\"] = new_episode_return\n                info[\"ice_got\"] = ice_got\n                info[\"episode_water\"] = episode_water\n\n                transition = Transition(\n                    done, action, value, reward, log_prob, last_obs, last_unipos, info\n                )\n                obsv = obsv_from_state(state.env_state)\n\n                runner_state = (train_state, state, obsv, rng)\n                return runner_state, transition\n\n            # runner_state0, transition0 = _env_step(runner_state, None)\n#             rich.print('End_of__env_step_test')\n\n            runner_state, traj_batch = jax.lax.scan(\n                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n            )\n            ###rich.print('traj_batch_obs',traj_batch.obs.shape)\n\n            # CALCULATE ADVANTAGE\n            train_state, state, last_obs, rng = runner_state\n            pi, last_val = network.apply(train_state.params, last_obs, state.env_state.units.pos.pos[:,0,:n_train_units])\n#             jax.debug.print(\"pi_probs: {}\", pi.probs)\n\n            def _calculate_gae(traj_batch, last_val):\n                def _get_advantages(gae_and_next_value, transition):\n                    gae, next_value = gae_and_next_value\n                    done, value, reward = (\n                        transition.done,\n                        transition.value,\n                        transition.reward,\n                    )\n                    # ###rich.print('transition', transition)\n                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n                    gae = (\n                        delta\n                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n                    )\n                    return (gae, value), gae\n\n                _, advantages = jax.lax.scan(\n                    _get_advantages,\n                    (jnp.zeros_like(last_val), last_val),\n                    traj_batch,\n                    reverse=True,\n                    unroll=16,\n                )\n                return advantages, advantages + traj_batch.value\n\n            advantages, targets = _calculate_gae(traj_batch, last_val)\n\n            # UPDATE NETWORK\n            def _update_epoch(update_state, unused):\n                def _update_minbatch(train_state, batch_info):\n                    traj_batch, advantages, targets = batch_info\n                    ###rich.print('batch_info',traj_batch.obs.shape)\n\n                    def _loss_fn(params, traj_batch, gae, targets):\n                        # RERUN NETWORK\n                        pi, value = network.apply(params, traj_batch.obs, traj_batch.last_unipos[:,0,:n_train_units])\n                        ###rich.print('traj_act2',jnp.repeat(traj_batch.action,2).reshape(-1,2))\n                        # jax.debug.print(\"traj_act2: {}\", jnp.repeat(traj_batch.action,2).reshape(-1,2))\n#                         log_prob = pi.log_prob(jnp.repeat(traj_batch.action,2).reshape(-1,2))[:,0]\n                        log_prob = pi.log_prob(traj_batch.action)[:,:n_train_units].sum(-1)\n                        # log_prob = pi.log_cdf(traj_batch.action)\n\n                        # CALCULATE VALUE LOSS\n                        value_pred_clipped = traj_batch.value + (\n                            value - traj_batch.value\n                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n                        value_losses = jnp.square(value - targets)\n                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n                        value_loss = (\n                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n                        )\n\n                        # CALCULATE ACTOR LOSS\n                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n                        loss_actor1 = ratio * gae\n                        loss_actor2 = (\n                            jnp.clip(\n                                ratio,\n                                1.0 - config[\"CLIP_EPS\"],\n                                1.0 + config[\"CLIP_EPS\"],\n                            )\n                            * gae\n                        )\n                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n                        loss_actor = loss_actor.mean()\n                        entropy = pi.entropy().mean()\n\n                        total_loss = (\n                            loss_actor\n                            + config[\"VF_COEF\"] * value_loss\n                            - config[\"ENT_COEF\"] * entropy\n                        )\n                        return total_loss, (value_loss, loss_actor, entropy)\n\n                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n                    total_loss, grads = grad_fn(\n                        train_state.params, traj_batch, advantages, targets\n                    )\n                    train_state = train_state.apply_gradients(grads=grads)\n                    return train_state, total_loss\n\n                train_state, traj_batch, advantages, targets, rng = update_state\n                rng, _rng = jax.random.split(rng)\n                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n                assert (\n                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n                ), \"batch size must be equal to number of steps * number of envs\"\n                permutation = jax.random.permutation(_rng, batch_size)\n                batch = (traj_batch, advantages, targets)\n                batch = jax.tree_util.tree_map(\n                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n                )\n                shuffled_batch = jax.tree_util.tree_map(\n                    lambda x: jnp.take(x, permutation, axis=0), batch\n                )\n                minibatches = jax.tree_util.tree_map(\n                    lambda x: jnp.reshape(\n                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n                    ),\n                    shuffled_batch,\n                )\n                # train_state0, total_loss0 = _update_minbatch(train_state, minibatches)\n                ##rich.print('End_of__update_minbatch_test')\n                train_state, total_loss = jax.lax.scan(\n                    _update_minbatch, train_state, minibatches\n                )\n                update_state = (train_state, traj_batch, advantages, targets, rng)\n                return update_state, total_loss\n\n            update_state = (train_state, traj_batch, advantages, targets, rng)\n            # update_state0, loss_info0 = _update_epoch(update_state,None)\n            ##rich.print('End_of__update_epoch_test')\n            update_state, loss_info = jax.lax.scan(\n                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n            )\n            train_state = update_state[0]\n            metric = traj_batch.info\n            metric['loss_info'] = loss_info[0]\n            rng = update_state[-1]\n            \n            # initialize env if done\n#             def true_fun():\n#                 # Code for the true branch\n#                 return state\n\n#             def false_fun():\n#                 # Code for the false branch\n#                 return init_env(rng)\n#             state = jax.lax.cond(jnp.bool_(jnp.min(state.episode_lengths)),true_fun,false_fun)\n            \n            state = init_env(rng) # always init_env after UPDATE\n        \n            runner_state = (train_state, state, last_obs, rng)\n            return runner_state, metric\n\n\n        obsv = obsv_from_state(state.env_state)\n        rng, _rng = jax.random.split(rng)\n        runner_state = (train_state, state, obsv, _rng)\n        # runner_state0, metric0 = _update_step(runner_state, None)\n        # rich.print('End_of__update_step_test')\n        if jit:\n            runner_state, metric = jax.lax.scan(\n                _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n            )\n        else:\n            for i in range(1):\n                runner_state, metric = jax.lax.scan(\n                    _update_step, runner_state, None, config[\"NUM_UPDATES\"]//5\n                )\n                rich.print(f'i= {i} , {config[\"NUM_UPDATES\"]//5*(i+1)} / {config[\"NUM_UPDATES\"]}')\n                rich.print(f'__ {round(time.time()-stime)} s')\n                out = {\"runner_state\": runner_state, \"metrics\": metric}\n                save_args = orbax_utils.save_args_from_target(out)\n                orbax_checkpointer.save(f'/kaggle/working/single_savetest_{i}', out, force=True, save_args=save_args)\n                return_updates_per_step = out['metrics']['new_episode_return'].mean(-1).max(-1)/out['metrics']['timestep'].mean(-1).max(-1)\n                rich.print('_____new_episode_return/step',\n                           return_updates_per_step)\n                water_updates = out['metrics']['episode_water'].mean(-1).max(-1)\n                rich.print('_____episode_water',\n                           water_updates)\n                plt.plot(return_updates_per_step.reshape(-1))\n                plt.plot(out[\"metrics\"][\"ice_got\"].mean(-1).mean(-1).reshape(-1)/4)\n                plt.plot(water_updates.reshape(-1))\n                plt.xlabel(\"Updates\")\n                plt.ylabel(\"Return\")\n                plt.legend(labels=(\n                    'new_episode_return',\n                    'ice_got/4',\n                    'water',\n                    ))\n                plt.show()\n#                 rich.print('loss_info',metric['loss_info'])\n                for u in range(len(metric['loss_info'])):\n                    plt.plot(np.stack(metric['loss_info'][u]).reshape(-1), label=f'loss_info_{u}')\n                plt.legend()\n                plt.show()\n#         train_state, state, last_obs, rng = runner_state\n\n        return {\"runner_state\": runner_state, \"metrics\": metric, \"config\": config}\n\n    return train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U jax # for orbax.checkpoint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import orbax.checkpoint\n# orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n# raw_restored = orbax_checkpointer.restore('/kaggle/working/single_savetest_2')\nraw_restored = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cpu 3e2:467s env2,step32,minib4\n# P100 2e5:310s env64,step32,minib4\nconfig = {\n    \"LR\": 5e-3,\n    \"NUM_ENVS\": 64,\n    \"NUM_STEPS\": 32,\n    \"TOTAL_TIMESTEPS\": 5e4, \n    \"UPDATE_EPOCHS\": 3,\n    \"NUM_MINIBATCHES\": 4,\n    \"GAMMA\": 0.99,\n    \"GAE_LAMBDA\": 0.95,\n    \"CLIP_EPS\": 0.2,\n    \"ENT_COEF\": 0.01,\n    \"VF_COEF\": 0.5,\n    \"MAX_GRAD_NORM\": 0.5,\n    # \"ACTIVATION\": \"relu\",\n    \"ANNEAL_LR\": True,\n}\nnum_features = 9\nnum_act = 12\nn_train_units = 1\nrng = jax.random.PRNGKey(36)\njit = False\n\njux_env_batch = JuxEnvBatch(env_cfg=EnvConfig(\n#     max_episode_length=40,\n    MIN_FACTORIES=n_train_units,\n    MAX_FACTORIES=n_train_units),\n    buf_cfg=JuxBufferConfig(MAX_N_UNITS=max_n_units))\n\njux_action = JuxAction.empty(\n    jux_env_batch.env_cfg,\n    jux_env_batch.buf_cfg\n)\nbatch_size = config['NUM_ENVS']\njux_action = jax.tree_map(lambda x: x[None].repeat(batch_size, axis=0), jux_action)\n\nimport time\nimport matplotlib.pyplot as plt\nimport orbax.checkpoint\nfrom flax.training import orbax_utils\norbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport optax\n\ndef objective(trial):\n    # Define hyperparameters to optimize\n    lr = trial.suggest_loguniform(\"LR\", 1e-7, 1e-2)\n#     num_envs = trial.suggest_int(\"NUM_ENVS\", 32, 128)\n    anneal_lr = trial.suggest_categorical(\"ANNEAL_LR\", [True, False])\n    # Add other hyperparameters to optimize here\n\n    # Update your configuration with the suggested hyperparameters\n    config[\"LR\"] = lr\n#     config[\"NUM_ENVS\"] = num_envs\n    config[\"anneal_lr\"] = anneal_lr\n\n    # Train your agent and return a metric to optimize (e.g., validation loss or reward)\n    train = make_train(config)\n    metric = train(rng)\n\n    return metric['metrics']['new_episode_return'].mean()  # Optuna will maximize/minimize this value\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")  # You can change the direction as needed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stime = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.optimize(objective, n_trials=10)  # You can change the number of trials","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = study.best_params\nbest_metric = study.best_value\nbest_params, best_metric","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_intermediate_values(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1):\n    stime = time.time()\n    rng = jax.random.PRNGKey(42+i)\n#     if i != 0:\n#         raw_restored = orbax_checkpointer.restore(f'/kaggle/working/single_savetest_{i-1}')\n    if raw_restored:\n        if jit:\n            train_jit = jax.jit(make_train(config, raw_restored))\n        else:\n            train_jit = make_train(config, raw_restored)\n    else:\n        if jit:\n            train_jit = jax.jit(make_train(config))\n        else:\n            train_jit = make_train(config)\n\n    out = train_jit(rng)\n    if jit:\n        save_args = orbax_utils.save_args_from_target(out)\n        orbax_checkpointer.save(f'/kaggle/working/single_savetest_{i}', out, force=True, save_args=save_args)\n        rich.print(f'step{i}, time, {round(time.time()-stime,0)}s')\n        return_updates_per_step = out['metrics']['new_episode_return'].mean(-1).max(-1)/out['metrics']['timestep'].mean(-1).max(-1)\n        rich.print('_____new_episode_return/step',\n                   return_updates_per_step)\n        water_updates = out['metrics']['episode_water'].mean(-1).max(-1)\n        rich.print('_____episode_water',\n                   water_updates)\n        plt.plot(return_updates_per_step.reshape(-1))\n        plt.plot(out[\"metrics\"][\"ice_got\"].mean(-1).mean(-1).reshape(-1)/10)\n        plt.plot(water_updates.reshape(-1))\n        plt.xlabel(\"Updates\")\n        plt.ylabel(\"Return\")\n        plt.legend(labels=(\n            'new_episode_return',\n            'ice_got/10',\n            'water',\n            ))\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}